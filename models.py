import streamlit as st
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM

@st.cache_resource
def load_embedding_model(model_name):
    """åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆé€‚é…Windows+Python3.11ï¼‰"""
    try:
        st.write(f"ğŸ“¥ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        model = SentenceTransformer(model_name)
        st.success(f"âœ… åµŒå…¥æ¨¡å‹ {model_name} åŠ è½½å®Œæˆï¼")
        return model
    except Exception as e:
        st.error(f"âŒ åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {str(e)}")
        return None

@st.cache_resource
def load_generation_model(model_name):
    """åŠ è½½ç”Ÿæˆæ¨¡å‹å’Œtokenizerï¼ˆé€‚é…Windows+Python3.11ï¼‰"""
    try:
        st.write(f"ğŸ“¥ æ­£åœ¨åŠ è½½ç”Ÿæˆæ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizerï¼ˆé€‚é…Qwenæ¨¡å‹ï¼‰
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹ï¼ˆWindows CPU/GPUè‡ªåŠ¨é€‚é…ï¼‰
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True
        )
        
        st.success(f"âœ… ç”Ÿæˆæ¨¡å‹ {model_name} åŠ è½½å®Œæˆï¼")
        return model, tokenizer
    except Exception as e:
        st.error(f"âŒ åŠ è½½ç”Ÿæˆæ¨¡å‹å¤±è´¥: {str(e)}")
        return None, None
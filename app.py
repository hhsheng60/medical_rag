# -*- coding: utf-8 -*-
import streamlit as st
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import pymilvus
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
import pandas as pd
import time

# ====================== ç¬¬ä¸€æ­¥ï¼šè®¾ç½®é¡µé¢é…ç½®ï¼ˆå¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªStreamlitå‘½ä»¤ï¼‰ ======================
st.set_page_config(
    page_title="åŒ»ç–—RAGé—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ====================== ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–å„ç±»æ¨¡å‹å’ŒMilvusè¿æ¥ ======================
# 1. åˆå§‹åŒ–BGE-m3å‘é‡æ¨¡å‹ï¼ˆä¼˜å…ˆåŠ è½½æœ¬åœ°ç¼“å­˜ï¼‰
@st.cache_resource
def init_bge_model():
    st.info("æ­£åœ¨åŠ è½½BGE-m3å‘é‡æ¨¡å‹...")
    try:
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„ï¼ˆæ›¿æ¢ä¸ºä½ å®é™…çš„BGEç¼“å­˜è·¯å¾„ï¼Œè‹¥æ²¡æœ‰åˆ™è‡ªåŠ¨ä¸‹è½½ï¼‰
        # ç¤ºä¾‹æœ¬åœ°è·¯å¾„ï¼šr"C:\Users\ä½ çš„ç”¨æˆ·å\.cache\torch\sentence_transformers\BAAI_bge-m3"
        model = SentenceTransformer('BAAI/bge-m3')
        st.success("BGE-m3å‘é‡æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model
    except Exception as e:
        st.error(f"BGEæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return None

# 2. åˆå§‹åŒ–Qwen-1.8B-Chatå¤§æ¨¡å‹ï¼ˆå·²ä¿®æ­£ä¸ºä½ çš„æ­£ç¡®è·¯å¾„ï¼‰
@st.cache_resource
def init_qwen_model():
    st.info("æ­£åœ¨åŠ è½½Qwen-1.8B-Chatå¤§æ¨¡å‹...")
    try:
        # ä½ çš„æ­£ç¡®Qwenæ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨rå‰ç¼€é¿å…è½¬ä¹‰ï¼‰
        model_path = model_path = r"D:\data-mining-knowledge-processing\medical_rag\models_cache\qwen-1_8b-chat"
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True,
            use_safetensors=True
        ).eval()
        st.success("Qwen-1.8B-Chatå¤§æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return tokenizer, model
    except Exception as e:
        st.error(f"Qwenæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return None, None

# 3. è¿æ¥Milvusæ•°æ®åº“
@st.cache_resource
def init_milvus():
    st.info("æ­£åœ¨è¿æ¥Milvusæ•°æ®åº“...")
    try:
        # æœ¬åœ°Milvusè¿æ¥ï¼ˆé»˜è®¤ç«¯å£19530ï¼Œè‹¥æœ‰ä¿®æ”¹è¯·å¯¹åº”è°ƒæ•´ï¼‰
        connections.connect(
            alias="default",
            host="127.0.0.1",
            port="19530"
        )
        # æ£€æŸ¥å¹¶åˆ›å»ºåŒ»ç–—æ–‡æ¡£é›†åˆï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        collection_name = "medical_docs"
        if not utility.has_collection(collection_name):
            # å®šä¹‰å­—æ®µ
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=2000),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1024)  # BGE-m3çš„å‘é‡ç»´åº¦
            ]
            schema = CollectionSchema(fields, description="åŒ»ç–—æ–‡æ¡£å‘é‡åº“")
            collection = Collection(name=collection_name, schema=schema)
            # åˆ›å»ºç´¢å¼•
            index_params = {
                "index_type": "IVF_FLAT",
                "metric_type": "COSINE",
                "params": {"nlist": 128}
            }
            collection.create_index(field_name="embedding", index_params=index_params)
            st.success("Milvusé›†åˆåˆ›å»ºæˆåŠŸï¼")
        else:
            collection = Collection(collection_name)
        st.success("Milvusæ•°æ®åº“è¿æ¥æˆåŠŸï¼")
        return collection
    except Exception as e:
        st.error(f"Milvusè¿æ¥å¤±è´¥ï¼š{str(e)}")
        return None

# ====================== ç¬¬ä¸‰æ­¥ï¼šæ ¸å¿ƒåŠŸèƒ½å‡½æ•° ======================
# å‘é‡æ£€ç´¢å‡½æ•°
def search_docs(query, bge_model, milvus_collection, top_k=3):
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_embedding = bge_model.encode([query])[0].tolist()
    # åŠ è½½é›†åˆå¹¶æ£€ç´¢
    milvus_collection.load()
    search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
    results = milvus_collection.search(
        data=[query_embedding],
        anns_field="embedding",
        param=search_params,
        limit=top_k,
        output_fields=["content"]
    )
    # æ•´ç†æ£€ç´¢ç»“æœ
    docs = []
    for hit in results[0]:
        docs.append(hit.entity.get("content"))
    return docs

# å¤§æ¨¡å‹å›ç­”ç”Ÿæˆå‡½æ•°
def generate_answer(query, docs, tokenizer, qwen_model):
    # æ„å»ºæç¤ºè¯
    prompt = f"""
    ä½ æ˜¯ä¸€åä¸“ä¸šçš„åŒ»ç–—é¡¾é—®ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    å‚è€ƒæ–‡æ¡£ï¼š
    {chr(10).join(docs)}
    
    ç”¨æˆ·é—®é¢˜ï¼š{query}
    
    å›ç­”è¦æ±‚ï¼š
    1. åŸºäºå‚è€ƒæ–‡æ¡£å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ï¼›
    2. è¯­è¨€é€šä¿—æ˜“æ‡‚ï¼Œç»“æ„æ¸…æ™°ï¼›
    3. å¦‚æœå‚è€ƒæ–‡æ¡£æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯´æ˜â€œæš‚æ— ç›¸å…³åŒ»ç–—ä¿¡æ¯â€ã€‚
    """
    # ç”Ÿæˆå›ç­”
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = qwen_model.generate(
            **inputs,
            max_new_tokens=500,
            temperature=0.7,
            top_p=0.95,
            do_sample=True
        )
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "").strip()
    return answer

# ====================== ç¬¬å››æ­¥ï¼šé¡µé¢UIå’Œäº¤äº’é€»è¾‘ ======================
def main():
    # åˆå§‹åŒ–ç»„ä»¶
    bge_model = init_bge_model()
    qwen_tokenizer, qwen_model = init_qwen_model()
    milvus_collection = init_milvus()

    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ¥ åŒ»ç–—RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    st.divider()

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.subheader("ç³»ç»Ÿé…ç½®")
        top_k = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", min_value=1, max_value=5, value=3)
        st.info("ç³»ç»Ÿå·²åŠ è½½ï¼š\n1. BGE-m3å‘é‡æ¨¡å‹\n2. Qwen-1.8B-Chatå¤§æ¨¡å‹\n3. Milvuså‘é‡æ•°æ®åº“")

    # ä¸»ç•Œé¢
    query = st.text_input("è¯·è¾“å…¥ä½ çš„åŒ»ç–—é—®é¢˜ï¼š", placeholder="ä¾‹å¦‚ï¼šé«˜è¡€å‹çš„æ—¥å¸¸æ³¨æ„äº‹é¡¹æœ‰å“ªäº›ï¼Ÿ")
    if st.button("è·å–å›ç­”", type="primary", disabled=(None in [bge_model, qwen_tokenizer, qwen_model, milvus_collection])):
        if not query.strip():
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ï¼")
        else:
            with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³åŒ»ç–—æ–‡æ¡£..."):
                docs = search_docs(query, bge_model, milvus_collection, top_k)
            st.subheader("ğŸ“š æ£€ç´¢åˆ°çš„å‚è€ƒæ–‡æ¡£")
            for i, doc in enumerate(docs, 1):
                st.write(f"{i}. {doc}")
            st.divider()
            with st.spinner("æ­£åœ¨ç”Ÿæˆä¸“ä¸šå›ç­”..."):
                answer = generate_answer(query, docs, qwen_tokenizer, qwen_model)
            st.subheader("ğŸ’¡ æ™ºèƒ½å›ç­”")
            st.write(answer)

if __name__ == "__main__":
    main()
    #streamlit run app.pyå¯åŠ¨
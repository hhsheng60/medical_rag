# app.py
import os
import json
import hashlib
import streamlit as st
from sentence_transformers import SentenceTransformer
from pymilvus import MilvusClient, CollectionSchema, FieldSchema, DataType

# ===================== é…ç½®é¡¹ =====================
# é…ç½® HuggingFace å›½å†…é•œåƒï¼ˆè§£å†³æ¨¡å‹ä¸‹è½½/åŠ è½½é—®é¢˜ï¼‰
os.environ["HF_ENDPOINT"] = os.getenv("HF_ENDPOINT", "https://hf-mirror.com")

# æ¨¡å‹é…ç½®ï¼ˆå’Œ preprocess.py ä¿æŒä¸€è‡´ï¼‰
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
EMBEDDING_KWARGS = {
    "normalize_embeddings": True,
    "prompt": "ä¸ºæ£€ç´¢ä»»åŠ¡ç”Ÿæˆè¡¨ç¤ºå‘é‡ï¼š"
}

# ========== Zilliz Cloud / Milvus äº‘æœåŠ¡é…ç½® ==========
MILVUS_URI = os.getenv(
    "MILVUS_URI",
    "https://in03-6c505f6fb47cb4f.serverless.aws-eu-central-1.cloud.zilliz.com"
)

# å¼ºçƒˆå»ºè®®æŠŠ token æ”¾åˆ°ç¯å¢ƒå˜é‡é‡Œï¼šMILVUS_TOKEN="xxxx"
MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "YOUR_MILVUS_TOKEN_HERE")

COLLECTION_NAME = os.getenv("COLLECTION_NAME", "medical_rag_collection")

# æ•°æ®è·¯å¾„
PROCESSED_DATA_PATH = "data/processed/processed_data.json"

# å…³é”®ï¼šMilvus/Zilliz çš„ VARCHAR ä¸Šé™æŒ‰ UTF-8 å­—èŠ‚è®¡ï¼ˆæœ€å¤§ 65535 bytesï¼‰
# å»ºè®®ç•™ä½™é‡ï¼Œé¿å…è¾¹ç•Œ/å¤šå­—èŠ‚å­—ç¬¦å¯¼è‡´è¶…é™
MAX_CONTENT_BYTES = 64000

# ä½ çš„ schema é‡Œ id max_length=64ï¼ˆåŒæ ·æŒ‰å­—èŠ‚è®¡ï¼‰ï¼Œé¿å… chunk_id è¶…é•¿
MAX_ID_BYTES = 64


# ===================== å·¥å…·å‡½æ•°ï¼ˆæŒ‰å­—èŠ‚ï¼‰ =====================
def utf8_bytes_len(s: str) -> int:
    return len((s or "").encode("utf-8"))


def safe_id(raw_id: str, max_bytes: int = MAX_ID_BYTES) -> str:
    """
    ç¡®ä¿ä¸»é”® id çš„ UTF-8 å­—èŠ‚é•¿åº¦ <= max_bytesã€‚
    è‹¥è¶…é•¿ï¼Œä½¿ç”¨â€œå‰ç¼€ + hashâ€ä¿è¯ç¨³å®šä¸”å”¯ä¸€æ€§è¾ƒé«˜ã€‚
    """
    raw_id = str(raw_id)
    if utf8_bytes_len(raw_id) <= max_bytes:
        return raw_id

    h = hashlib.md5(raw_id.encode("utf-8")).hexdigest()[:10]
    prefix = raw_id[:30]
    candidate = f"{prefix}_{h}"

    # æç«¯æƒ…å†µä¸‹å†æˆªæ–­
    b = candidate.encode("utf-8")
    if len(b) <= max_bytes:
        return candidate
    return b[:max_bytes].decode("utf-8", errors="ignore")


def split_text_by_utf8_bytes(text: str, max_bytes: int = MAX_CONTENT_BYTES):
    """
    æŒ‰ UTF-8 å­—èŠ‚ä¸Šé™åˆ‡åˆ†ï¼Œä¿è¯æ¯ä¸ª chunk çš„ utf8 å­—èŠ‚æ•° <= max_bytes
    ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾å®šä½æ¯æ®µæœ€å¤§åˆ‡ç‰‡ä½ç½®ï¼Œé¿å…ä¸­æ–‡å¤šå­—èŠ‚æº¢å‡ºã€‚
    """
    text = text or ""
    chunks = []
    start = 0
    n = len(text)

    while start < n:
        lo, hi = start + 1, n
        best = lo
        while lo <= hi:
            mid = (lo + hi) // 2
            if utf8_bytes_len(text[start:mid]) <= max_bytes:
                best = mid
                lo = mid + 1
            else:
                hi = mid - 1

        chunk = text[start:best]
        if chunk.strip():
            chunks.append(chunk)
        start = best

    return chunks


# ===================== åˆå§‹åŒ–å‡½æ•° =====================
@st.cache_resource
def init_model():
    """åˆå§‹åŒ– BGE-m3 æ¨¡å‹ï¼ˆç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰"""
    st.info("æ­£åœ¨åŠ è½½ BGE-m3 æ¨¡å‹...")
    model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    st.success("BGE-m3 æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model


@st.cache_resource
def init_milvus():
    """åˆå§‹åŒ– Milvus/Zilliz å®¢æˆ·ç«¯ï¼Œå¹¶åˆ›å»ºé›†åˆ + ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    if MILVUS_TOKEN == "YOUR_MILVUS_TOKEN_HERE":
        st.warning("ä½ è¿˜æ²¡æœ‰è®¾ç½® MILVUS_TOKEN ç¯å¢ƒå˜é‡ï¼Œå°†æ— æ³•æ­£å¸¸è¿æ¥äº‘æœåŠ¡ã€‚")

    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)
    st.success("Milvus/Zilliz äº‘æœåŠ¡å®¢æˆ·ç«¯è¿æ¥æˆåŠŸï¼")

    # å®šä¹‰é›†åˆ schemaï¼ˆVARCHAR æœ€å¤§ 65535 bytesï¼‰
    fields = [
        FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
        FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1024),
    ]
    schema = CollectionSchema(fields=fields, description="åŒ»ç–— RAG æ•°æ®é›†ï¼ˆé•¿æ–‡æ¡£åˆ†å—ï¼‰")

    if not client.has_collection(collection_name=COLLECTION_NAME):
        client.create_collection(collection_name=COLLECTION_NAME, schema=schema)

        client.create_index(
            collection_name=COLLECTION_NAME,
            field_name="embedding",
            index_params={
                "index_type": "IVF_FLAT",
                "metric_type": "COSINE",
                "params": {"nlist": 128},
            },
        )
        st.info(f"é›†åˆ {COLLECTION_NAME} åˆ›å»ºå®Œæˆï¼Œå¹¶å·²å»ºç«‹ç´¢å¼•ã€‚")
    else:
        st.info(f"é›†åˆ {COLLECTION_NAME} å·²å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨ã€‚")

    return client


def load_data_to_milvus(client: MilvusClient, model: SentenceTransformer):
    """
    åŠ è½½é¢„å¤„ç†åçš„æ•°æ®åˆ° Milvusï¼š
    - ä»¥ UTF-8 å­—èŠ‚ä¸ºå‡†åˆ¤æ–­æ˜¯å¦è¶…é•¿
    - è¶…é•¿åˆ™æŒ‰ UTF-8 å­—èŠ‚å®‰å…¨åˆ†å—ï¼Œå¹¶å¯¹æ¯ä¸ª chunk é‡æ–°ç”Ÿæˆå‘é‡
    """
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
    try:
        stats = client.get_collection_stats(collection_name=COLLECTION_NAME)
        row_count = stats.get("row_count", 0)
        if isinstance(row_count, str) and row_count.isdigit():
            row_count = int(row_count)
        if row_count and row_count > 0:
            st.info(f"Milvus ä¸­å·²æœ‰ {row_count} æ¡è®°å½•ï¼Œè·³è¿‡åŠ è½½ã€‚")
            return
    except Exception as e:
        st.warning(f"è·å–æ•°æ®é‡å¤±è´¥ï¼š{e}ï¼Œç»§ç»­å°è¯•åŠ è½½ã€‚")

    if not os.path.exists(PROCESSED_DATA_PATH):
        st.error(f"é¢„å¤„ç†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{PROCESSED_DATA_PATH}")
        st.stop()

    with open(PROCESSED_DATA_PATH, "r", encoding="utf-8") as f:
        processed_data = json.load(f)

    if not processed_data:
        st.warning("é¢„å¤„ç†æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åŠ è½½ã€‚")
        return

    insert_data = []
    total_chunks = 0

    with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£ï¼ˆæŒ‰ UTF-8 å­—èŠ‚åˆ†å—ï¼‰..."):
        for doc in processed_data:
            original_id = safe_id(doc.get("id", ""))
            original_content = doc.get("content", "")
            original_embedding = doc.get("embedding", None)

            # 1) çŸ­æ–‡æ¡£ï¼šæŒ‰å­—èŠ‚åˆ¤æ–­ï¼Œç›´æ¥æ’å…¥ï¼ˆä¿æŒåŸ embeddingï¼‰
            if utf8_bytes_len(original_content) <= MAX_CONTENT_BYTES:
                if original_embedding is None:
                    # å…œåº•ï¼šå¦‚æœæ²¡æœ‰ embeddingï¼Œå°±ç°åœºç”Ÿæˆ
                    original_embedding = model.encode(original_content, **EMBEDDING_KWARGS).tolist()

                insert_data.append(
                    {
                        "id": original_id,
                        "content": original_content,
                        "embedding": original_embedding,
                    }
                )
                total_chunks += 1

            # 2) é•¿æ–‡æ¡£ï¼šæŒ‰å­—èŠ‚å®‰å…¨åˆ†å—ï¼Œchunk é‡æ–°ç¼–ç å‘é‡
            else:
                st.info(
                    f"æ–‡æ¡£ {original_id} UTF-8 å­—èŠ‚é•¿åº¦ {utf8_bytes_len(original_content)}ï¼Œè‡ªåŠ¨åˆ†å—..."
                )
                chunks = split_text_by_utf8_bytes(original_content, MAX_CONTENT_BYTES)

                for idx, chunk in enumerate(chunks):
                    chunk_id = safe_id(f"{original_id}_c{idx}")
                    chunk_embedding = model.encode(chunk, **EMBEDDING_KWARGS).tolist()

                    # æœ€ç»ˆå®‰å…¨æ ¡éªŒï¼ˆé˜²æ­¢ä»»ä½•æ„å¤–è¶…é™ï¼‰
                    if utf8_bytes_len(chunk) > 65535:
                        # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼Œå› ä¸º MAX_CONTENT_BYTES < 65535
                        chunk = chunk.encode("utf-8")[:65535].decode("utf-8", errors="ignore")

                    insert_data.append(
                        {
                            "id": chunk_id,
                            "content": chunk,
                            "embedding": chunk_embedding,
                        }
                    )
                    total_chunks += 1

    with st.spinner("æ­£åœ¨æ’å…¥æ•°æ®åˆ° Milvus/Zilliz..."):
        # å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œå»ºè®®åˆ†æ‰¹æ’å…¥ï¼ˆè¿™é‡Œç»™ä¸€ä¸ªå®‰å…¨çš„ batchï¼‰
        batch_size = 256
        for i in range(0, len(insert_data), batch_size):
            client.insert(
                collection_name=COLLECTION_NAME,
                data=insert_data[i : i + batch_size],
            )

    st.success(
        f"æˆåŠŸåŠ è½½ {total_chunks} ä¸ªæ–‡æœ¬å—åˆ° Milvusï¼ˆåŸæ–‡æ¡£æ•°ï¼š{len(processed_data)}ï¼‰ï¼"
    )


def search_similar_docs(client: MilvusClient, model: SentenceTransformer, query: str, top_k: int = 3):
    """æ£€ç´¢ç›¸ä¼¼æ–‡æœ¬å—"""
    query_embedding = model.encode(query, **EMBEDDING_KWARGS).tolist()

    results = client.search(
        collection_name=COLLECTION_NAME,
        data=[query_embedding],
        limit=top_k,
        output_fields=["id", "content"],
        search_params={"metric_type": "COSINE", "params": {"nprobe": 10}},
    )

    similar_docs = []
    for res in results[0]:
        doc_id = res["entity"]["id"]
        is_chunk = "_c" in doc_id or "_chunk" in doc_id
        similarity = round(1 - res["distance"], 4)  # COSINE è·ç¦»è½¬ç›¸ä¼¼åº¦ï¼ˆè¿‘ä¼¼ç”¨æ³•ï¼‰

        similar_docs.append(
            {
                "id": doc_id,
                "content": res["entity"]["content"],
                "similarity": similarity,
                "is_chunk": is_chunk,
            }
        )

    return similar_docs


# ===================== Streamlit å‰ç«¯ =====================
def main():
    st.set_page_config(
        page_title="åŒ»ç–—RAGé—®ç­”ç³»ç»Ÿï¼ˆBGE-m3 + é•¿æ–‡æ¡£åˆ†å—ï¼‰",
        page_icon="ğŸ¥",
        layout="wide",
    )

    st.title("ğŸ¥ åŒ»ç–—RAGé—®ç­”ç³»ç»Ÿï¼ˆåŸºäº BGE-m3 å‘é‡æ¨¡å‹ + é•¿æ–‡æ¡£åˆ†å—ï¼‰")

    # åˆå§‹åŒ–
    model = init_model()
    client = init_milvus()

    # åŠ è½½æ•°æ®ï¼ˆä»…é¦–æ¬¡ï¼‰
    load_data_to_milvus(client, model)

    # é—®ç­”äº¤äº’
    st.divider()
    query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š", placeholder="ä¾‹å¦‚ï¼šé«˜è¡€å‹çš„æ—¥å¸¸æ³¨æ„äº‹é¡¹ï¼Ÿ")
    top_k = st.slider("æ£€ç´¢ç›¸ä¼¼æ–‡æœ¬å—æ•°é‡ï¼š", min_value=1, max_value=5, value=3)

    if st.button("æ£€ç´¢ç­”æ¡ˆ", type="primary"):
        if not query.strip():
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ï¼")
            return

        with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸ä¼¼æ–‡æœ¬å—..."):
            similar_docs = search_similar_docs(client, model, query, top_k)

        st.subheader("ğŸ“ ç›¸ä¼¼æ–‡æœ¬å—æ£€ç´¢ç»“æœ")
        if not similar_docs:
            st.info("æœªæ£€ç´¢åˆ°ç›¸ä¼¼æ–‡æœ¬å—ã€‚")
        else:
            for idx, doc in enumerate(similar_docs, 1):
                chunk_note = "ï¼ˆé•¿æ–‡æ¡£åˆ†å—ï¼‰" if doc["is_chunk"] else ""
                with st.expander(
                    f"æ–‡æœ¬å— {idx}ï¼ˆç›¸ä¼¼åº¦ï¼š{doc['similarity']:.4f}ï¼‰{chunk_note}"
                ):
                    st.write(f"æ–‡æœ¬å—IDï¼š{doc['id']}")
                    st.write(doc["content"])

        st.subheader("ğŸ’¡ é—®ç­”ç»“æœ")
        if similar_docs:
            st.write(similar_docs[0]["content"])
        else:
            st.write("æš‚æ— ç›¸å…³ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    main()
